# Optimizing an ML Pipeline in Azure
## Table of Contents
* ### Overview
* ### Summary
  * Problem Statement
  * Solution
* ### Pipelines
  * #### Scikit-learn Pipeline with Hyperdrive
     * Pipeline Architecture
     * Classification Algorithm
     * Parameter Sampler
     * Early Stopping Policy
  * #### AutoML Pipeline
* ### Pipelines Comparison
* ### Future Work

## Overview
This project is a part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.
![Screenshot](automl_pipeline.png)

## Summary
**Problem Statement** 
The dataset contains data about marketing campaigns for a bank, the campaigns are based on phone calls. We seek to predict whether a bank product would be subscribed by the client or not (yes or no).
The dataset contains 32950 training examples in a csv file.
**Solution**
We had two approaches to solving the problem, the first approach was by using Hyperdrive to obtain the best values of the hyperparamters for a scikit-learn logistic regression model, in order to maximize the accuracy of the model.
The second approach, was to use Azure's automl to find the best performing model based on the highest accuracy value.

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**
* We first need to prepare our train.py script by:
 * Importing the csv file containing the marketing campaigns data into our dataset using the TabularDataSetFactory module.
 * Cleaning the dataset, which included droping NaN values.
 * Splitting our dataset into training set (80% of the data) & test set (20% of the data.) 
 * Creating a Logistic Regression model using sci-kit learn.
 * Creating a directory(outputs) to save the generated model into it.
* After the train.py script is ready, we choose a proper parameter sampling method for the inverse regularization paramter(C) & the maximum number of iterations(max_iter), early termination policy and an estimator to create the HyperDriveConfig.
 * The HyperDriveConfig was configured using the following:
                             the estimator we created for the train.py,
                             Paramater sampling method chosen,
                             The early termination policy chosen,
                             primary_metric_name, which is the Accuracy,
                             primary_metric_goal, which is to maximize the primary metric,
                             max_total_runs=4,
                             max_concurrent_runs=4
* Then we submit the hyperdrive run.
* Once the run is complete, we choose the best run (the run that achieved the maximum accuracy) and save the model generated.
 The best value of the Accuracy was found to be: **0.9072837632776934**
The following diagram summarizes the workflow:
![Scikit-learn Pipeline](https://github.com/dinaabdulrasoul/optimizing-an-ml-pipeline/blob/master/hyperdrive_pipeline.PNG)
**Algorithm** 
Logistic Regression is a supervisied binary classification algorithm that predicts the probability of a target varaible, returning either 1 or 0 (yes or no).

**Parameter Sampler**
For this pipeline, Random sampling has been used. 
Random Sampling is a great sampler to avoid bias, and it also supports early termination of low-performance runs.

**Early Stopping Policy**
For this pipeline, Bandit Policy has been used, which is an early termination policy based on slack criteria, and the evaluation interval. 
* Slack_factor is the ratio used to calculate the allowed distance from the best performing experiment run.
* Evaluation_interval is the frequency for applying the policy.
*The benefits of this stopping policy* is that any run that doesn't fall within the slack factor or slack amount of the evaluation metric with respect to the best performing run will be terminated so this helps us quickly eliminate the bad performing runs.

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**
The best performing model found by the AutoML was the Voting Ensemble with an accuracy of 0.91779.
V
The best run details based on the choosen automl configuration can be viewed in this snapshot from the Azure Portal:

The following diagram summarizes the Pipeline workflow:
![AutoML Pipeline](https://github.com/dinaabdulrasoul/optimizing-an-ml-pipeline/blob/master/Automl_pipeline.PNG)

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**
The scikit-learn model, with the use of hyperdrive for hyperparameters tuning, achieved an accuracy of 0.9072837632776934, while the automl model ahieved an accuracy of 0.91779.

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**
Some of the imporvements might be using a different sampling paramater, for example: BayesianSampling or GridSampling, they might take up more time and resources however that might improve the accuracy.
Also I can try not using a termination policy, for example with BayesianSampling method as the data we have is not that large so an early termination policy is not really necessary. 
.....
